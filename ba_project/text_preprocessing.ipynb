{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This file is used for preprocessing twitter data\n",
    "# Saving into relevant data shape and data cleaning\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_non_alphanum\n",
    "from gensim.parsing.preprocessing import strip_numeric, strip_punctuation, strip_short\n",
    "\n",
    "# import gensim\n",
    "# print(gensim.parsing.preprocessing.STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count total token words in a DataFrame or a column\n",
    "def count_words(var):\n",
    "    if isinstance(var, pd.Series):\n",
    "        tokens = 0\n",
    "        for items in var.iteritems():\n",
    "            tokens += len(items[1].split())\n",
    "        return tokens\n",
    "    elif isinstance(var, pd.DataFrame):\n",
    "        return var.count().sum()\n",
    "    \n",
    "# Map POS tag to first character lemmatize() accepts\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Custom functionfor text lemmatization and removing non-dictionary words\n",
    "def lemmatize_custom(my_list, cwords):\n",
    "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    \n",
    "    for i in cwords:\n",
    "        words.add(i)\n",
    "        \n",
    "    tweets_ll = []\n",
    "    for item in my_list:\n",
    "        word_list = item.split()\n",
    "        word_list = [lemma.lemmatize(x, get_wordnet_pos(x)) for x in word_list]\n",
    "        word_list = [x for x in word_list if x in words or not x.isalpha()]\n",
    "        tweets_ll.append(word_list)\n",
    "        word_list = None\n",
    "    \n",
    "    return pd.DataFrame(tweets_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing raw tweets data from csv\n",
    "\n",
    "tweets = pd.read_csv(\"tweets_raw.csv\", index_col=0)\n",
    "words_earlier = count_words(tweets[\"Text\"])\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Removing links and ampersand attached text from the tweets\n",
    "tweets_text = [re.sub(r\"(?:\\@|\\&|http)\\S+\", \"\", item) for item in tweets[\"Text\"]]\n",
    "\n",
    "# Removing non-alphabetic and numeric characters\n",
    "tweets_text = [strip_numeric(item) for item in tweets_text]\n",
    "tweets_text = [strip_non_alphanum(item) for item in tweets_text]\n",
    "\n",
    "# Removing punctuation characters\n",
    "tweets_text = [strip_punctuation(item) for item in tweets_text]\n",
    "\n",
    "# Short words removal, minsize 3\n",
    "tweets_text = [strip_short(item, minsize=3) for item in tweets_text]\n",
    "\n",
    "# All text to lower case\n",
    "tweets_text = [item.lower() for item in tweets_text]\n",
    "\n",
    "# Removing the stopwords from the tweets\n",
    "tweets_text = [remove_stopwords(item) for item in tweets_text]\n",
    "                                   \n",
    "# Remove everything except text\n",
    "# tweets_text[\"text\"] = [re.sub(r\"[^a-zA-Z]+\", ' ', item) for item in tweets_text[\"text\"]]\n",
    "# tweets_text[\"text\"] = [re.sub(r\"[^a-zA-Z0-9]+\", ' ', item) for item in tweets_text[\"text\"]]\n",
    "\n",
    "pd.DataFrame(tweets_text, columns=[\"Text\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting words in the text into tokens and putting into a dataframe\n",
    "# Each row has useful words from a single tweet (like a transaction)\n",
    "\n",
    "custom_words = [\"lockdown\"]\n",
    "tweets_tokenized = lemmatize_custom(tweets_text, custom_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_after = count_words(tweets_tokenized)\n",
    "print(\"Words before preprocessing: {}\".format(words_earlier))\n",
    "print(\"Words after preprocessing: {}\".format(words_after))\n",
    "print(\"Words removed: {}\".format(words_earlier-words_after))\n",
    "\n",
    "tweets_tokenized.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_tokenized.to_csv(\"tweets_tokenized_py.csv\", index = False, header = True)\n",
    "# tweets_tokenized.index += 1\n",
    "# tweets_tokenized.to_csv(\"tweets_tokenized_r.csv\", index = True, header = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
