{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This file is used for preprocessing twitter data\n",
    "# Saving into relevant data shape and data cleaning\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_non_alphanum\n",
    "from gensim.parsing.preprocessing import strip_numeric, strip_punctuation, strip_short\n",
    "\n",
    "# import gensim\n",
    "# print(gensim.parsing.preprocessing.STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count total token words in a DataFrame or a column\n",
    "def count_words(var):\n",
    "    if isinstance(var, pd.Series):\n",
    "        tokens = 0\n",
    "        for items in var.iteritems():\n",
    "            tokens += len(items[1].split())\n",
    "        return tokens\n",
    "    elif isinstance(var, pd.DataFrame):\n",
    "        return var.count().sum()\n",
    "    \n",
    "# Map POS tag to first character lemmatize() accepts\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Custom functionfor text lemmatization and removing non-dictionary words\n",
    "def lemmatize_custom(my_list, cwords):\n",
    "    start = time.time()\n",
    "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    \n",
    "    with open(\"behaviour_words.txt\",\"r\") as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "    behaviour_words = []\n",
    "    for i in lines:\n",
    "        for ix in i.split():\n",
    "            behaviour_words.append(ix)\n",
    "    cwords = cwords + behaviour_words\n",
    "    \n",
    "    for i in cwords:\n",
    "        words.add(i)\n",
    "        \n",
    "    tweets_ll = []\n",
    "    for item in my_list:\n",
    "        word_list = item.split()\n",
    "        # word_list = [lemma.lemmatize(x, get_wordnet_pos(x)) for x in word_list]\n",
    "        # word_list = [x for x in word_list if x in words]\n",
    "        word_list = [x for x in word_list if x in cwords]\n",
    "        # word_list = [x for x in word_list if x in behaviour_words]\n",
    "        tweets_ll.append(word_list)\n",
    "        word_list = None\n",
    "    end = time.time()\n",
    "    print(\"Total time taken in lemmatization: {:.2f} seconds\".format(end-start))\n",
    "    return pd.DataFrame(tweets_ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data importing from our saved csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1217\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>HashTags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>@XiaomiIndia @Xiaomi we hve ordered Mi 40\" TV ...</td>\n",
       "      <td>2020-04-01 03:59:59+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@XiaomiIndia @Xiaomi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>@OnlineGpsc I had purchased my online class fr...</td>\n",
       "      <td>2020-04-01 05:48:42+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@OnlineGpsc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>@SamsungIndia @Samsung @amazon Worst experienc...</td>\n",
       "      <td>2020-04-01 06:05:48+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@SamsungIndia @Samsung @amazon @amazon</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>@Xiaomi Mi A2 Packed on 2018, July Purchased O...</td>\n",
       "      <td>2020-04-01 06:27:55+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@Xiaomi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>@flipkartsupport recently just before the coun...</td>\n",
       "      <td>2020-04-01 08:17:43+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@flipkartsupport</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  \\\n",
       "629  @XiaomiIndia @Xiaomi we hve ordered Mi 40\" TV ...   \n",
       "532  @OnlineGpsc I had purchased my online class fr...   \n",
       "209  @SamsungIndia @Samsung @amazon Worst experienc...   \n",
       "628  @Xiaomi Mi A2 Packed on 2018, July Purchased O...   \n",
       "688  @flipkartsupport recently just before the coun...   \n",
       "\n",
       "                          Date  Retweets  Favorites  \\\n",
       "629  2020-04-01 03:59:59+00:00         0          0   \n",
       "532  2020-04-01 05:48:42+00:00         0          0   \n",
       "209  2020-04-01 06:05:48+00:00         0          0   \n",
       "628  2020-04-01 06:27:55+00:00         0          0   \n",
       "688  2020-04-01 08:17:43+00:00         0          0   \n",
       "\n",
       "                                   Mentions HashTags  \n",
       "629                    @XiaomiIndia @Xiaomi      NaN  \n",
       "532                             @OnlineGpsc      NaN  \n",
       "209  @SamsungIndia @Samsung @amazon @amazon      NaN  \n",
       "628                                 @Xiaomi      NaN  \n",
       "688                        @flipkartsupport      NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing raw tweets data from csv\n",
    "\n",
    "tweets = pd.read_csv(\"tweets_raw.csv\", index_col=0)\n",
    "words_earlier = count_words(tweets[\"Text\"])\n",
    "print(\"Number of rows: {}\".format(tweets.shape[0]))\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hve ordered purchased emi bank started emi ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>purchased online class phone samsung phone dam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worst experience lockdown bought device receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>packed july purchased october months got packe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recently country wide lockdown purchased realm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  hve ordered purchased emi bank started emi ind...\n",
       "1  purchased online class phone samsung phone dam...\n",
       "2  worst experience lockdown bought device receiv...\n",
       "3  packed july purchased october months got packe...\n",
       "4  recently country wide lockdown purchased realm..."
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing links and ampersand attached text from the tweets\n",
    "tweets_text = [re.sub(r\"(?:\\@|\\&|http)\\S+\", \"\", item) for item in tweets[\"Text\"]]\n",
    "\n",
    "# Removing non-alphabetic and numeric characters\n",
    "tweets_text = [strip_numeric(item) for item in tweets_text]\n",
    "tweets_text = [strip_non_alphanum(item) for item in tweets_text]\n",
    "\n",
    "# Removing punctuation characters\n",
    "tweets_text = [strip_punctuation(item) for item in tweets_text]\n",
    "\n",
    "# Short words removal, minsize 3\n",
    "tweets_text = [strip_short(item, minsize=3) for item in tweets_text]\n",
    "\n",
    "# All text to lower case\n",
    "tweets_text = [item.lower() for item in tweets_text]\n",
    "\n",
    "# Removing the stopwords from the tweets\n",
    "tweets_text = [remove_stopwords(item) for item in tweets_text]\n",
    "                                   \n",
    "# Remove everything except text\n",
    "# tweets_text[\"text\"] = [re.sub(r\"[^a-zA-Z]+\", ' ', item) for item in tweets_text[\"text\"]]\n",
    "# tweets_text[\"text\"] = [re.sub(r\"[^a-zA-Z0-9]+\", ' ', item) for item in tweets_text[\"text\"]]\n",
    "\n",
    "pd.DataFrame(tweets_text, columns=[\"Text\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding sentiment label of tweets\n",
    "with open(\"lexicon/positive.txt\", \"r\") as f:\n",
    "    positive = [line.strip() for line in f]\n",
    "with open(\"lexicon/negative.txt\", \"r\") as f:\n",
    "    negative = [line.strip() for line in f]\n",
    "\n",
    "tweets_senti = []\n",
    "for item in tweets_text:\n",
    "    word_list = item.split()\n",
    "    p = 0; n = 0; neu = 0\n",
    "    for i in word_list:\n",
    "        if i in positive:\n",
    "            p+=1\n",
    "        elif i in negative:\n",
    "            n+=1\n",
    "        else:\n",
    "            neu+=1\n",
    "    if p>=n:\n",
    "        tweets_senti.append(\"positive\")\n",
    "    elif n>p:\n",
    "        tweets_senti.append(\"negative\")\n",
    "    \n",
    "    word_list = None\n",
    "    \n",
    "sentiment_labels = pd.Series(tweets_senti, name = \"sentiment\")\n",
    "sentiment_labels.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnning our custom lemmatization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken in lemmatization: 0.16 seconds\n"
     ]
    }
   ],
   "source": [
    "# Converting words in the text into tokens and putting into a dataframe\n",
    "# Each row has useful words from a single tweet (like a transaction)\n",
    "\n",
    "# custom_words = [\"lockdown\"]\n",
    "custom_words = []\n",
    "tweets_tokenized = lemmatize_custom(tweets_text, custom_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words before preprocessing: 46849\n",
      "Words after preprocessing: 2482\n",
      "Words removed: 44367\n",
      "Number of rows: 1013\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>purchased</td>\n",
       "      <td>samsung</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>purchased</td>\n",
       "      <td>realme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>samsung</td>\n",
       "      <td>purchased</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>think</td>\n",
       "      <td>honor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>want</td>\n",
       "      <td>buy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1    2    3    4    5\n",
       "1  purchased    samsung  NaN  NaN  NaN  NaN\n",
       "4  purchased     realme  NaN  NaN  NaN  NaN\n",
       "5    samsung  purchased  NaN  NaN  NaN  NaN\n",
       "6      think      honor  NaN  NaN  NaN  NaN\n",
       "7       want        buy  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_after = count_words(tweets_tokenized)\n",
    "print(\"Words before preprocessing: {}\".format(words_earlier))\n",
    "print(\"Words after preprocessing: {}\".format(words_after))\n",
    "print(\"Words removed: {}\".format(words_earlier-words_after))\n",
    "\n",
    "tweets_tokenized.dropna(axis = 0,how = 'all',inplace = True, thresh=2)\n",
    "\n",
    "df1 = tweets_tokenized.stack().reset_index().drop(columns='level_1').drop_duplicates()\n",
    "df1['col'] = df1.groupby('level_0').cumcount()\n",
    "tweets_tokenized = (df1.pivot(index='level_0', columns='col', values=0)\n",
    "          .rename_axis(index=None, columns=None))\n",
    "\n",
    "print(\"Number of rows: {}\".format(tweets_tokenized.shape[0]))\n",
    "tweets_tokenized.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_tokenized.to_csv(\"tweets_tokenized.csv\", index = False, header = True)\n",
    "# tweets_tokenized.index += 1\n",
    "# tweets_tokenized.to_csv(\"tweets_tokenized_r.csv\", index = True, header = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
